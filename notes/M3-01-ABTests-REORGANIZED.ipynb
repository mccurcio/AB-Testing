{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0474b3c7",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#AB-Testing-Design\" data-toc-modified-id=\"AB-Testing-Design-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>AB Testing Design</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-A/B-testing-and-when-to-use-it?\" data-toc-modified-id=\"What-is-A/B-testing-and-when-to-use-it?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>What is A/B testing and when to use it?</a></span></li><li><span><a href=\"#Questions-to-ask-before-any-A/B-test\" data-toc-modified-id=\"Questions-to-ask-before-any-A/B-test-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Questions to ask before any A/B test</a></span></li><li><span><a href=\"#Choosing-a-primary-metric-for-the-A/B-test\" data-toc-modified-id=\"Choosing-a-primary-metric-for-the-A/B-test-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Choosing a primary metric for the A/B test</a></span></li><li><span><a href=\"#Common-A/B-test-metrics\" data-toc-modified-id=\"Common-A/B-test-metrics-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Common A/B test metrics</a></span></li><li><span><a href=\"#Stating-the-hypothesis-of-the-test\" data-toc-modified-id=\"Stating-the-hypothesis-of-the-test-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Stating the hypothesis of the test</a></span></li><li><span><a href=\"#Designing-the-A/B-test\" data-toc-modified-id=\"Designing-the-A/B-test-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Designing the A/B test</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Statistical-Hypothesis\" data-toc-modified-id=\"Step-1:-Statistical-Hypothesis-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Step 1: Statistical Hypothesis</a></span></li><li><span><a href=\"#Step-2:-Power-Analysis\" data-toc-modified-id=\"Step-2:-Power-Analysis-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Step 2: Power Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Power-of-the-Test\" data-toc-modified-id=\"Power-of-the-Test-1.6.2.1\"><span class=\"toc-item-num\">1.6.2.1&nbsp;&nbsp;</span>Power of the Test</a></span></li><li><span><a href=\"#Significance-level-of-the-test\" data-toc-modified-id=\"Significance-level-of-the-test-1.6.2.2\"><span class=\"toc-item-num\">1.6.2.2&nbsp;&nbsp;</span>Significance level of the test</a></span></li><li><span><a href=\"#Minimum-Detectable-Effect-(d)-????\" data-toc-modified-id=\"Minimum-Detectable-Effect-(d)-????-1.6.2.3\"><span class=\"toc-item-num\">1.6.2.3&nbsp;&nbsp;</span>Minimum Detectable Effect (d) ????</a></span></li></ul></li><li><span><a href=\"#Step-3:-Calculating-minimum-sample-size\" data-toc-modified-id=\"Step-3:-Calculating-minimum-sample-size-1.6.3\"><span class=\"toc-item-num\">1.6.3&nbsp;&nbsp;</span>Step 3: Calculating minimum sample size</a></span></li><li><span><a href=\"#Case-1:-Sample-Size-Calculation-with-Binary-Metric\" data-toc-modified-id=\"Case-1:-Sample-Size-Calculation-with-Binary-Metric-1.6.4\"><span class=\"toc-item-num\">1.6.4&nbsp;&nbsp;</span>Case 1: Sample Size Calculation with Binary Metric</a></span></li><li><span><a href=\"#Case-2:-Sample-Size-Calculation-with-Continuous-Metric\" data-toc-modified-id=\"Case-2:-Sample-Size-Calculation-with-Continuous-Metric-1.6.5\"><span class=\"toc-item-num\">1.6.5&nbsp;&nbsp;</span>Case 2: Sample Size Calculation with Continuous Metric</a></span></li><li><span><a href=\"#Too-small-test-duration:-Novelty-Effects\" data-toc-modified-id=\"Too-small-test-duration:-Novelty-Effects-1.6.6\"><span class=\"toc-item-num\">1.6.6&nbsp;&nbsp;</span>Too small test duration: Novelty Effects</a></span></li><li><span><a href=\"#Too-large-test-duration:-Maturation-Effects\" data-toc-modified-id=\"Too-large-test-duration:-Maturation-Effects-1.6.7\"><span class=\"toc-item-num\">1.6.7&nbsp;&nbsp;</span>Too large test duration: Maturation Effects</a></span></li></ul></li><li><span><a href=\"#Running-the-A/B-test\" data-toc-modified-id=\"Running-the-A/B-test-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Running the A/B test</a></span></li></ul></li><li><span><a href=\"#Analyzing-A/B-test-results-with-Python\" data-toc-modified-id=\"Analyzing-A/B-test-results-with-Python-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Analyzing A/B test results with Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#Choosing-an-appropriate-statistical-test\" data-toc-modified-id=\"Choosing-an-appropriate-statistical-test-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Choosing an appropriate statistical test</a></span></li><li><span><a href=\"#2-sample-T-test\" data-toc-modified-id=\"2-sample-T-test-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>2-sample T-test</a></span></li><li><span><a href=\"#2-sample-Z-test\" data-toc-modified-id=\"2-sample-Z-test-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>2-sample Z-test</a></span></li><li><span><a href=\"#Case-1:-Z-test-for-comparing-proportions-(2-sided)\" data-toc-modified-id=\"Case-1:-Z-test-for-comparing-proportions-(2-sided)-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Case 1: Z-test for comparing proportions (2-sided)</a></span></li><li><span><a href=\"#Case-2:-Z-test-for-comparing-means-(2-sided)\" data-toc-modified-id=\"Case-2:-Z-test-for-comparing-means-(2-sided)-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Case 2: Z-test for comparing means (2-sided)</a></span></li><li><span><a href=\"#Statistical-Significance-vs-Practical-Significance\" data-toc-modified-id=\"Statistical-Significance-vs-Practical-Significance-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Statistical Significance vs Practical Significance</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77056d3-fc39-4442-838e-e994ef715b4e",
   "metadata": {},
   "source": [
    "## AB Testing Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada51630-8f93-4210-812a-771f1d4a4cd5",
   "metadata": {},
   "source": [
    "A/B testing originated from the randomized control trials in Statistics. It is synonymous with split testing. A/B testing is a popular way for businesses to test new UX features. New versions of a product, or an algorithm can be tested to decide whether a business should launch a new product or feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee84378-7f14-42d4-bf76-feec60023ebd",
   "metadata": {},
   "source": [
    "### What is A/B testing and when to use it?\n",
    "\n",
    "The motivation behind A/B testing is to determine if new product variants improve the performance of the existing product. Essentially we ask, will make this product be more successful than its predecessor.\n",
    "\n",
    "The idea of A/B testing is quite simple. \n",
    "\n",
    "1. Show a new variation of the product to a sample number of customers, the experimental group. \n",
    "\n",
    "\n",
    "2. Then show a current unchanged version to another sample of customers, the control group. \n",
    "\n",
    "\n",
    "3. The rate of product performance between the treatment group versus the control group is tracked.\n",
    "\n",
    "\n",
    "4. The effect of the new product version(s) over the performance of unchanged product is  measured over time. The versions are tracked for a sufficiently long test period.\n",
    "\n",
    "What makes this testing great is that businesses get direct feedback from real customers and users. Testing new variants and new ideas can be carried out quickly. \n",
    "\n",
    "The A/B test shows if a variation is or is not effective and then decide whether they need to improve their products or investigate other areas and ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5351d8-1331-47d8-9801-faf00f2c40c2",
   "metadata": {},
   "source": [
    "### Questions to ask before any A/B test\n",
    "\n",
    "- A/B tests usually require a significant amount of resources. \n",
    "\n",
    "- A/B tests may result in product decisions with a significant impact. \n",
    "- Therefore, it is highly important to incorporate *product teams, engineering teams, and other stakeholders* before testing.\n",
    "\n",
    "\n",
    "- Questions need to be worked through in advance:\n",
    "   - What does a sample population look like and what are the customer segments for the target product?\n",
    "   - Can we find alternative answers to our business question using exploratory or historical data analysis?\n",
    "   - Do we want to test single or multiple variants of the target product?\n",
    "   - Can we ensure truly randomized control and experimental groups such that, both samples are an unbiased and fairly represent the user population?\n",
    "   - Can we ensure the integrity of the treatment vs control effects during the entire duration of the test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1532c5-f0c5-4eea-a6b3-600cfd21f022",
   "metadata": {},
   "source": [
    "### Choosing a primary metric for the A/B test\n",
    "\n",
    "- The goal is to identify whether there is a statistically significant difference between these two groups.\n",
    "\n",
    "\n",
    "- Therefore, choosing the 'right' metric is important in A/B testing since it will be used throughout testing and analysis. \n",
    "\n",
    "\n",
    "- Testers need to find a metric that closely correlates with the higher-level goals of the project. \n",
    "\n",
    "\n",
    "- Revenue may not always the end goal. \n",
    "\n",
    "\n",
    "- There is a trade off with respect to the material's content. The expectation is that if the product makes more money, then the content is good. However, instead of improving the overall material content, one could optimize the conversion funnels. \n",
    "\n",
    "You can ask yourself the following **question**:\n",
    "\n",
    "- Metric Validity Question: If this chosen metric were to increase significantly while everything else stays constant, would we achieve our goal and address the problem?\n",
    "- One method to test the accuracy of your metric is to return the original problem you want to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffb983-9ede-49a2-9cba-4fe4bc54dc59",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Common A/B test metrics\n",
    "Popular performance metrics that are often used in A/B testing are:\n",
    "1. Click Through Rate, \n",
    "2. Click Through Probability,\n",
    "3. Conversion Rate.\n",
    "\n",
    "\n",
    "1. **Click-Through Rate** (CTR) for usage: This number is the clicks per views of the page.\n",
    "\n",
    "$$\\text{Click-Through Rate} = 100\\% \\cdot \\frac{Clicks}{views}$$\n",
    "\n",
    "2. **Click-Through Probability** ([CTP](https://yokk.medium.com/differences-between-click-through-rate-ctr-and-click-through-probabilities-ctp-7f7d89d5526f)) for impact: A probability of how well users interact with a button, keyword, or ads. The CTP considers duplicate clicks of an item. If a user responds with multiple clicks on an item, in a single session, for any reason (for example impatience), the multiple clicks are counted only as one click in CTP.\n",
    "\n",
    "$$\\text{Click-Through Probability} = 100\\% \\cdot \\frac{\\text{# of people with at least one click}}{\\text{# of unique visitors}}$$\n",
    "\n",
    "3. **Conversion Rate**: Conversion rate, defined as the proportion of sessions ending up with a transaction.\n",
    "\n",
    "$$\\text{Conversion Rate} = 100\\% \\cdot \\frac{\\text{# of converted}}{\\text{# of converted + # of not converted}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca6aac-66a7-4f7c-bc39-5a36d38d0f7b",
   "metadata": {},
   "source": [
    "### Stating the hypothesis of the test\n",
    "\n",
    "- A/B test should always be based on a hypothesis that needs to be tested. This starting point is usually set as a result of brainstorming and collaboration of relevant people on the product teams and Data Science team. \n",
    "\n",
    "- One benefit of first producing a hypothesis is to decide how to ‘fix’ any potential issues.\n",
    "\n",
    "- It is important to understand that problems may greatly influence the Key Performance Indicators (**KPIs**) of interest?\n",
    "\n",
    "For example, if the **KPIs** of the product are to improve the quality of a recommender engine or system’s recommendations and this can be done for example by adding Impression Discounting or building a Re-ranker model for the recommender. \n",
    "\n",
    "However, the impact of these two solutions will likely be different on the amount of improvement of the quality of the recommendations. Namely, the re-ranker model affects the ranks of the recommendations by potentially changing the set of recommendations presented to the user, unlike impression discounting which just makes sure that the user doesn’t see the recommendations that were previously viewed to the user.\n",
    "\n",
    "For this particular example, we could decide to build a re-ranked model which we expect to improve the quality of the target recommender (let’s name this imaginary recommender system, RecSys). \n",
    "\n",
    "Hypothesis: Adding an XGBoost re-ranker model to the existing RecSys recommender will increase the CTR of the recommendations, that is, will improve the quality of the RecSys recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be535e-b70d-4f03-a54e-c278a606f7b8",
   "metadata": {},
   "source": [
    "### Designing the A/B test\n",
    "Following are the steps you need to take to have a solid design for your A/B test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf84822-fb65-4ab7-a271-68ddf757c1cd",
   "metadata": {},
   "source": [
    "#### Step 1: Statistical Hypothesis\n",
    "\n",
    "$\n",
    "  \\begin{cases}\n",
    "    H_0:& p_{con}=p_{exp}\\\\\n",
    "    H_1: &p_{con}\\neq p_{exp}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "$\n",
    "  \\begin{cases}\n",
    "    H_0:& \\mu_{con}=\\mu_{exp}\\\\\n",
    "    H_1: &\\mu_{con}\\neq \\mu_{exp}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "- $H_0$: The null hypotheis states that the mean($\\mu$) or proportion($p$) are the same and the test shows no significant change between the control vs treatment sample.\n",
    "\n",
    "- $H_1$: While the alternative hypothesis states that the means($\\mu$) or proportions($p$) of the treatment and control groups are significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efee5d6-a076-426a-bb46-1dbd6cb9b63b",
   "metadata": {},
   "source": [
    "#### Step 2: Power Analysis\n",
    "\n",
    "- To ensure statistical significant results and avoid bias, we want to collect “enough” observations and run the test for at least a minimum, predetermined amount of time.\n",
    "   \n",
    "Therefore, before running the test we need to determine:   \n",
    "1. The sample size of the control and experimental groups, and\n",
    "2. How long we need to run the test. \n",
    "   \n",
    "- This process is often referred to as Power Analysis and it includes 3 specific steps: \n",
    "1. Determine: [Power of the test](https://en.wikipedia.org/wiki/Power_of_a_test), \n",
    "2. Determine: [Significance level of the test](https://en.wikipedia.org/wiki/Statistical_significance),\n",
    "3. Determine: [Minimum Detectable Effect](https://splitmetrics.com/resources/minimum-detectable-effect-mde/).\n",
    "\n",
    "- These tests are determined to make sure that the results are repeatable, robust, and can be generalized to the entire population. If these tests are not considered the common issue of 'P-hacking' can easily occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e20b9-c0bc-44ef-8423-b8f1708cc314",
   "metadata": {},
   "source": [
    "##### Power of the Test\n",
    "\n",
    "- Power is the probability of making a correct decision (to reject the null hypothesis) when the null hypothesis is false.\n",
    "\n",
    "$$Power ~=~ Pr \\left( rejecting~ the~ H_0 ~|~ H_1 ~is~ true \\right)$$\n",
    "\n",
    "- Power is usually given as a percent. For example, it is common practice to pick 80% as the power of the A/B test, that is 20% Type II error which means that we are fine with not detecting (failing to reject the null) a treatment effect while there is an effect. \n",
    "     \n",
    "     \n",
    "- The power, often defined by (1 - $\\beta$) is equal to the probability of not making a type II error, where Type II error is the probability of not rejecting the null hypothesis while the null is false."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1feac-66e3-404a-a3e8-d640fb10a2ff",
   "metadata": {},
   "source": [
    "##### Significance level of the test\n",
    "\n",
    "The significance level which is also the probability of Type I error is the likelihood of rejecting the null, hence detecting a treatment effect, while the null is true and there is no statistically significant impact. This value, often defined by the Greek letter alpha, α, is the probability of making a false discovery often referred to as a false-positive rate.\n",
    "    \n",
    "Generally, we use the significance value of 5% which indicates that we have a 5% risk of concluding that there exists a statistically significant difference between the experimental and control variant performances when there is no actual difference. So, we are fine by having 5 out of 100 cases detecting a treatment effect while there is no effect. It also means that you have a significant result difference between the control and the experimental groups with a 95% confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1052788-cd50-4ccd-b56c-a75e96d8e409",
   "metadata": {},
   "source": [
    "##### Minimum Detectable Effect (d) ????\n",
    "\n",
    "   - From a business point of view, what is the substantive to the statistical significance that the business wants to see as a minimum impact of the new version to find this variant investment-worthy?\n",
    "    \n",
    "   - The answer to this question is what form the amount of change we aim to observe in the new version’s metric compared to the existing one to make recommendations to the business that this feature should be launched in the production. An estimate of this parameter is what is known as the Minimum Detectable Effect, often defined by the Greek letter delta, which is also related to the practical significance of the test. MDE is a proxy that relates to the smallest effect that would matter in practice for the business and is usually set by stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433cd59e-a19e-4e19-ab48-439c22c9c8a7",
   "metadata": {},
   "source": [
    "#### Step 3: Calculating minimum sample size\n",
    "\n",
    "- Determining the minimum sample size of the control and experimental groups, is very important. \n",
    "\n",
    "- The sample size needs to be determined using the defined power of the test (1-beta), the significance level (alpha), Minimum Detectable Effect (MED), and the variances of the two Normally Distributed samples of equal size. Calculation of the sample size depends on the underlying primary metric that you have chosen for tracking the progress of the control and experimental versions. Here we distinguish two cases; case 1 where the primary metric of A/B testing is in the form of a binary variable (e.g. click or no click) and case 2 where the primary metric of the test is in the form of proportions or averages (e.g. mean order amount).\n",
    "\n",
    "**NOTE**: Lehr's (approximation) rule of thumb says that the sample size n for a two-sided two-sample t-test with power = 80% ($\\beta$ = 0.2 and significance level $\\alpha$ = 0.05 approximates:\n",
    "$$\\large n ~=~~ 16 \\cdot \\frac{s^2}{d^2}$$\n",
    "Where:\n",
    "\n",
    "- $s^2$ is an estimate of the population variance \n",
    "- $d = \\mu _{1}~-~\\mu _{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37be269-47b3-46c3-b905-e3213b9d4f54",
   "metadata": {},
   "source": [
    "#### Case 1: Sample Size Calculation with Binary Metric\n",
    " \n",
    "   When we are dealing with a primary performance tracking metric that has two possible values such as the Click Through Rate where the user can either click (success) or not click (failure) and if the users’ responses to the product can be defined as “independent” events then we can consider this as Bernoulli Trials where the click event (success) occurs with probability p_con in case of the Control Group and p_exp in case of the Experimental Group.\n",
    "   \n",
    "$$N = \\frac{\\left(Z_{1-\\frac{\\alpha}{2}}\\cdot sd_1 + Z_{1-\\beta}\\cdot sd_2 \\right)^2}{d^2}$$\n",
    "   \n",
    "   \n",
    "    where:  \n",
    "\n",
    "- $sd_1 = \\sqrt{2 \\cdot p(1-p)}$ , $sd_2 = \\sqrt{p(1-p)+(p+d)(1-(1-(p+d))}$, ????\n",
    "- $d = \\mu _{1}~-~\\mu _{2}$\n",
    "- $p$ : is the proportion of succesful occurences, i.e. clicks divided by the total\n",
    "- $1 ~-~ p$ : is the fraction of failures, (Not clicking) divided by the total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaef67d1-1a78-458e-9524-435730ee437e",
   "metadata": {},
   "source": [
    "####  Case 2: Sample Size Calculation with Continuous Metric\n",
    "When we are dealing with a primary performance tracking metric that is in the form of an average such as the mean order amount where we intend to compare the means of the Control and Experimental Groups, then we can use the Central Limit Theorem and state that the mean sampling distribution of both Control and Experimental Groups follow Normal Distribution. Consequently, the sampling distributions of difference of means of these two groups also follow Normal Distribution. That is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681b7e0-0487-4859-be48-dd1401730a8a",
   "metadata": {},
   "source": [
    "$\\bar X_{con} \\sim N(\\mu_{con},\\sigma_{con}^2)$\n",
    "\n",
    "$\\bar X_{exp} \\sim N(\\mu_{exp},\\sigma_{exp}^2)$\n",
    "\n",
    "$\\bar X_{con}-\\bar X_{con} \\sim N \\left(\\mu_{con}-\\mu_{exp},\\frac{\\sigma_{con}^2}{N_{con}^2}+\\frac{\\sigma_{exp}^2}{N_{exp}^2} \\right)$\n",
    "\n",
    "$N=\\frac{\\big(\\sigma_{con}^2 ~~+~~ \\sigma_{exp}^2 \\big) \\left(Z_{1-\\frac{\\alpha}{2}} ~~+~~ Z_{1-\\beta} \\right)}{d^2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794c8a4-5fe0-4716-92f2-791a8b70b339",
   "metadata": {},
   "source": [
    "Following the industry standards, \n",
    "\n",
    "if $\\alpha=0.05$, where $Z_{1-\\frac{\\alpha}{2}}=1.96$ and $\\beta=0.2$ where $Z_{\\beta}=0.84$, and \n",
    "\n",
    "assuming that $\\sigma^2=\\sigma_{con}^2=\\sigma_{exp}^2$, we have:\n",
    "\n",
    "$N=\\frac{2\\sigma^2\\times \\left(1.96+0.84 \\right)^2}{d^2} \\approx \\frac{16\\sigma^2}{d^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f98fc-3bf5-42c4-bba5-8a7d3b0584a4",
   "metadata": {},
   "source": [
    "   - Smaller $\\alpha$--> more confidence level --> more number of samples\n",
    "   - Smaller $\\beta$--> More Power ($1-\\beta$)--> more number of samples\n",
    "   - Smaller variance--> less number of samples\n",
    "   - Smaller delta (d) --> more number of samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eac724-024d-4f9b-aa58-801a83a4aae7",
   "metadata": {},
   "source": [
    "- Step 4 : Determining A/B test duration\n",
    "As mentioned before, this question needs to be answered before you run your experiment not during, by trying to stop the test when you detect statistical significance. To determine the baseline of a duration time, a common approach is to use the following formula:\n",
    "\n",
    "\n",
    "$\\text{Duration} = \\frac{N}{\\text{# of visitors per day}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455f85a-5197-4e2b-994b-0d82090a6e1d",
   "metadata": {},
   "source": [
    "#### Too small test duration: Novelty Effects\n",
    "\n",
    "Users react quickly and positively to all types of changes independent of their nature. This positive effect to the experimental version is referred to as a novelty effect and it wears off over time. This postive effect is considered “illusory.” An initial postive effect is temporary and one would be careful to expect that it will continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31404da2-f783-458d-896e-865b4a984d85",
   "metadata": {},
   "source": [
    "#### Too large test duration: Maturation Effects\n",
    "An initial spike of interest may be due to the novelty effect of the treatment. \n",
    "\n",
    "The longer the test period is the larger the likelihood of external effects impacting the reaction of the users and possibly contaminating the test results, maturation effect. \n",
    "\n",
    "Therefore, running the A/B test for too long is not recommended and should be avoided. This will ultimately increase the reliability of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee562cfb-ef99-478b-9f41-982b35551790",
   "metadata": {},
   "source": [
    "### Running the A/B test\n",
    "Once the preparation work has been done, with the help of engineering you can start running the A/B test. Firstly, the engineering team needs to make sure that the integrity between Control and Experimental groups is kept. Secondly, the mechanism storing users’ responses to the treatment has to be accurate and the same across all users to avoid systematic bias. There are also few things you want to avoid doing such as stopping the test too early once you detect statistical significance (small p-value) while you have not reached the minimum sample size calculated before starting the test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456cfef2-5c34-4408-afb5-d79dfde0081b",
   "metadata": {},
   "source": [
    "## Analyzing A/B test results with Python\n",
    "When it comes to interpreting the results of your A/B test, there is a set of values you should calculate to test the statistical hypothesis stated earlier (to test whether there is a statistically significant difference between control and experimental groups). This set includes:\n",
    "- Choosing an appropriate statistical test\n",
    "- Calculating the test statistics (T)\n",
    "- Calculating the p-value of the test statistics\n",
    "- Reject or fail to reject the statistical hypothesis (statistical significance)\n",
    "- Calculate the margin of error (external validity of the experiment)\n",
    "- Calculate confidence interval (external validity and practical significance of the experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ea6e5-0a4f-48c8-9c46-2f0eb3135317",
   "metadata": {},
   "source": [
    "### Choosing an appropriate statistical test\n",
    "Once the interaction data of the Control and Experimental groups are collected, you can test the statistical hypothesis earlier by choosing an appropriate statistical test that is usually categorized in parametric and non-parametric tests. The choice of the test depends on the following factors:\n",
    "\n",
    "   - format of the primary metric (underlying pdf)\n",
    "   - sample size (for CLT)\n",
    "   - nature of the statistical hypothesis (show that a relationship between two groups merely exists or identify the type of relationship between the groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375ccec-b657-4dae-b262-8f5c25351ac7",
   "metadata": {},
   "source": [
    "The most popular parametric tests that are used in A/B testing are:\n",
    "- 2 Sample T-test (when N < 30, metric follows student-t distribution, and you want to identify whether there exist a relationship and the type of relationship between control and experimental groups)\n",
    "- 2 Sample Z-test (when N > 30, metric follows asymptotic Normal distribution and you want to identify whether there exist a relationship and the type of relationship between control and experimental groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a273305-4324-4f30-bd19-43ebb925e0c8",
   "metadata": {},
   "source": [
    "The most popular non-parametric tests that are used in A/B testing are:\n",
    "- Fishers Exact test (small N, identify and you want to identify whether there exist a relationship between control and experimental groups)\n",
    "- Chi-Squared test (large N, identify and you want to identify whether there exist a relationship between control and experimental groups)\n",
    "- Wilcoxon Rank Sum/Mann Whitney test (small N or large N, skewed sampling distributions, testing the difference in medians between control and experimental groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a46d6-0612-4658-a5c8-b4399cd617d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2-sample T-test\n",
    "If you want to test whether there is a statistically significant difference between the control and experimental groups’ metrics that are in the form of averages (e.g. average purchase amount), metric follows student-t distribution and when the sample size is smaller than 30, you can use 2-sample T-test to test the following hypothesis:\n",
    "\n",
    "$\n",
    "  \\begin{cases}\n",
    "    H_0:& \\mu_{con}=\\mu_{exp}\\\\\n",
    "    H_1: &\\mu_{con}\\neq \\mu_{exp}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3074edfe-5467-4cd2-955b-b1fde2044e74",
   "metadata": {},
   "source": [
    "where the sampling distribution of means of Control and Experiment group follows Student-t distribution with degrees of freedom N_con-1 and N_exp-1, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57f4a5d-a0e0-4de9-b140-0fd9cd868435",
   "metadata": {},
   "source": [
    "Then an estimate for the pooled variance of the two samples can be calculated as follows:\n",
    "\n",
    "$\\large \\hat s_{p}^2=\\frac{ \\left( N_{con}-1 \\right) \\sigma_{con}^2+\\left(N_{exp}-1\\right)\\sigma_{exp}^2}{\\left(N_{con}+N_{exp}-2 \\right)}\\left(\\frac{1}{N_{con}}+\\frac{1}{N_{exp}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b88daac-df12-4f6e-9a1a-c736a5ed7882",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xbar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25673/1764237393.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxbar\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSE\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu_diff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu_con\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmu_exp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_diff\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xbar' is not defined"
     ]
    }
   ],
   "source": [
    "t=(xbar-mu)/sqrt(SE/N)\n",
    "xbar=mu_diff=mu_con-mu_exp\n",
    "mu=0\n",
    "\n",
    "t=(mu_diff-0)/SD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82546a0e-7908-4dc9-bb10-5384628a74d5",
   "metadata": {},
   "source": [
    "Consequently, the test statistics of the 2-sample T-test with the hypothesis stated earlier can be calculated as follows:\n",
    "\n",
    "$$T=\\frac{\\hat \\mu_{con}-\\hat \\mu_{exp}}{\\sqrt{\\hat s_p^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59e851-c2a1-4458-9a05-49d676e5221c",
   "metadata": {},
   "source": [
    "In order to test the statistical significance of the observed difference between sample means, we need to calculate the p-value of our test statistics. The p-value is the probability of observing values at least as extreme as the common value when this is due to a random chance.\n",
    "\n",
    "$$p_{value} = Pr[t\\leq T \\text{ or } t\\geq T]=2 \\times Pr[t \\geq T] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e5612c-d997-4adb-9790-c8a93c7eb92a",
   "metadata": {},
   "source": [
    "Finally, to determine how accurate the obtained results are and also to comment about the practical significance of the obtained results, you can compute the Confidence Interval of your test by using the following formula:\n",
    "\n",
    "$\\large CI = [(\\mu_{con}-\\mu_{exp})-t_{1-\\frac{\\alpha}{2}}\\times SE\\text{ , } (\\mu_{con}-\\mu_{exp})+t_{1-\\frac{\\alpha}{2}}\\times SE ]$\n",
    "\n",
    "where the $t_{(1-\\alpha/2)}$ is the critical value of the test corresponding to the two-sided t-test with alpha significance level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e94a1d3-6001-438d-8cbc-11187684cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "N_con = 20\n",
    "df_con = N_con - 1 # degrees of freedom of Control \n",
    "N_exp = 20\n",
    "df_exp = N_exp - 1 # degrees of freedom of Experimental \n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# data of control group with t-distribution\n",
    "X_con = np.random.standard_t(df_con,N_con)\n",
    "# data of experimental group with t-distribution\n",
    "X_exp = np.random.standard_t(df_exp,N_exp)\n",
    "\n",
    "# mean of control\n",
    "mu_con = np.mean(X_con)\n",
    "# mean of experimental\n",
    "mu_exp = np.mean(X_exp)\n",
    "\n",
    "# variance of control\n",
    "sigma_sqr_con = np.var(X_con)\n",
    "#variance of control\n",
    "sigma_sqr_exp = np.var(X_exp)\n",
    "\n",
    "# pooled variance\n",
    "pooled_variance_t_test = ((N_con-1)*sigma_sqr_con + (N_exp -1) * sigma_sqr_exp)/(N_con + N_exp-2)*(1/N_con + 1/N_exp)\n",
    "\n",
    "# Standard Error\n",
    "SE = np.sqrt(pooled_variance_t_test)\n",
    "\n",
    "# Test Statistics\n",
    "T = (mu_con-mu_exp)/SE\n",
    "\n",
    "# Critical value for two sided 2 sample t-test\n",
    "t_crit = t.ppf(1-alpha/2, N_con + N_exp - 2)\n",
    "\n",
    "# P-value of the two sided T-test using t-distribution and its symmetric property\n",
    "p2 = (1-t.cdf(T, N_con + N_exp - 2))*2\n",
    "p_value = t.sf(T, N_con + N_exp - 2)*2 #1-cdf\n",
    "\n",
    "# Margin of Error\n",
    "margin_error = t_crit * SE\n",
    "# Confidence Interval\n",
    "CI = [(mu_con-mu_exp) - margin_error, (mu_con-mu_exp) + margin_error]\n",
    "\n",
    "print(\"T-score: \", T)\n",
    "print(\"T-critical: \", t_crit)\n",
    "print(p2)\n",
    "print(\"P_value: \", p_value)\n",
    "print(\"Confidence Interval of 2 sample Z-test: \", np.round(CI,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae66472-af6a-491e-8641-8acdb5f59f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "-2.02, -0.36, mucon-muexp=0, 0.36, 2.02\n",
    "\n",
    "if p>significance level\n",
    "or\n",
    "CI includes the zero point\n",
    "we fail to reject tjhe null hypothesis\n",
    "or, we dont have a sstatistically significant diferrence between the\n",
    "contorl and treatment groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3d377-5ac8-43f4-978b-580a43746b20",
   "metadata": {},
   "source": [
    "### 2-sample Z-test\n",
    "If you want to test whether there is a statistically significant difference between the control and experimental groups’ metrics that are in the form of averages (e.g. average purchase amount) or proportions (e.g. Click Through Rate), metric follows Normal distribution, or when the sample size is larger than 30 such that you can use Central Limit Theorem (CLT) to state that the sampling distributions of Control and Experimental groups are asymptotically Normal, you can use 2-sample Z-test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465b3f9d-f991-47e7-a913-ac7452f0acd0",
   "metadata": {},
   "source": [
    "### Case 1: Z-test for comparing proportions (2-sided)\n",
    "If you want to test whether there is a statistically significant difference between the Control and Experimental groups’ metrics that are in the form of proportions (e.g. CTR) and if the click event occurs independently, you can use a 2-sample Z-test to test the following hypothesis:\n",
    "\n",
    "$$\n",
    "  \\begin{cases}\n",
    "    H_0:& p_{con}=p_{exp}\\\\\n",
    "    H_1: &p_{con}\\neq p_{exp}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02dc795-a864-4295-993a-12d22e3ffb86",
   "metadata": {},
   "source": [
    "after collecting the interaction data of the Control and Experimental users, you can calculate the estimates of these two probabilities as follows:\n",
    "\n",
    "$ \\large \\hat p_{control}=\\frac{X_{control}}{N_{control}}=\\frac{\\text{# clicks}_{control}}{\\text{# impressions}_{control}}$\n",
    "\n",
    "$ \\large \\hat p_{experimental}=\\frac{X_{experimental}}{N_{experimental}}=\\frac{\\text{# clicks}_{experimental}}{\\text{# impressions}_{experimental}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37884717-eda4-493c-a6da-584b0cfb20e2",
   "metadata": {},
   "source": [
    "Since we are testing for the difference in these probabilities, we need to obtain an estimate for the pooled probability of success and an estimate for pooled variance, which can be done as follows:\n",
    "\n",
    "$$\\hat p_{p}=\\frac{X_{control}+X_{exp}}{N_{control}+N_{exp}}$$\n",
    "\n",
    "$$\\hat s_p^2 =\\hat p_p (1-\\hat p_p)\\cdot \\left(\\frac{1}{N_{control}}+\\frac{1}{N_{exp}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe0df6d-6335-4bf4-ab55-8c9cc50df1e8",
   "metadata": {},
   "source": [
    "Consequently, the test statistics of the 2-sample Z-test for the difference in proportions can be calculated as follows:\n",
    "\n",
    "$$Z=\\frac{\\hat p_{con}-\\hat p_{exp}}{\\sqrt{\\hat s_p^2}}$$\n",
    "\n",
    "$$p_{value} = Pr[t\\leq Z \\text{ or } t\\geq Z]=2 \\times Pr[t \\geq Z] $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c94d7-67ff-4db3-8b29-632a81664504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "X_con = 1242 #clicks control\n",
    "N_con = 9886 #impressions control\n",
    "X_exp = 974 #clicks experimental\n",
    "N_exp = 10072 #impressions experimetal\n",
    "\n",
    "# Significance Level\n",
    "alpha = 0.05\n",
    "\n",
    "p_con_hat = X_con / N_con\n",
    "p_exp_hat = X_exp / N_exp\n",
    "\n",
    "p_pooled_hat = (X_con + X_exp)/(N_con + N_exp)\n",
    "pooled_variance = p_pooled_hat*(1-p_pooled_hat) * (1/N_con + 1/N_exp)\n",
    "\n",
    "# Standard Error\n",
    "SE = np.sqrt(pooled_variance)\n",
    "\n",
    "# test statsitics\n",
    "Test_stat = (p_con_hat - p_exp_hat)/SE\n",
    "# critical value usig the standard normal distribution\n",
    "Z_crit = norm.ppf(1-alpha/2)\n",
    "\n",
    "# Margin of error\n",
    "m = SE * Z_crit\n",
    "# two sided test and using symmetry property of Normal distibution so we multiple with 2\n",
    "p_value = norm.sf(Test_stat)*2\n",
    "\n",
    "# Confidence Interval\n",
    "CI = [(p_con_hat-p_exp_hat) - SE * Z_crit, (p_con_hat-p_exp_hat) + SE * Z_crit]\n",
    "\n",
    "if np.abs(Test_stat) >= Z_crit:\n",
    "    print(\"reject the null\")\n",
    "    print(p_value)\n",
    "\n",
    "print(\"Test Statistics stat: \", Test_stat)\n",
    "print(\"Z-critical: \", Z_crit)\n",
    "print(\"P_value: \", p_value)\n",
    "print(\"Confidence Interval of 2 sample Z-test for proportions: \", np.round(CI,2))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "z = np.arange(-3,3,  0.1)\n",
    "plt.plot(z, norm.pdf(z), label = 'Standard Normal Distribution',color = 'purple',linewidth = 2.5)\n",
    "plt.fill_between(z[z>Z_crit], norm.pdf(z[z>Z_crit]), label = 'Right Rejection Region',color ='y' )\n",
    "plt.fill_between(z[z<(-1)*Z_crit], norm.pdf(z[z<(-1)*Z_crit]), label = 'Left Rejection Region',color ='y' )\n",
    "plt.title(\"Two Sample Z-test rejection region\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f3b36-bff5-490d-8fed-e0421510d00e",
   "metadata": {},
   "source": [
    "### Case 2: Z-test for comparing means (2-sided)\n",
    "If you want to test whether there is a statistically significant difference between the Control and Experimental groups’ metrics that are in the form of averages (e.g. CTR) you can use a 2-sample Z-test to test the following hypothesis:\n",
    "\n",
    "$$\n",
    "  \\begin{cases}\n",
    "    H_0:& \\mu_{con}=p_{exp}\\\\\n",
    "    H_1: &\\mu_{con}\\neq p_{exp}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f3287-f1ea-40ad-8f36-e13a770cd921",
   "metadata": {},
   "source": [
    "where the sampling distribution of means of Control and Experiment group follows Normal distribution with mean $\\mu_{con}$ $\\mu_{exp}$ and $\\sigma_{con}^2/N_{con}$ $\\sigma_{exp}^2/N_{exp}$, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e919e882-c2ba-46d6-a6b4-09b88fd2f248",
   "metadata": {},
   "source": [
    "The difference in the means of the control and experimental groups also follows Normal distribution:\n",
    "\n",
    "$$\\hat \\mu_{con}-\\hat \\mu_{exp}~N \\left(\\mu_{con}-\\mu_{exp}, \\frac{\\sigma_{con}^2}{N_{con}}+\\frac{\\sigma_{exp}^2}{N_{exp}} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be668972-22bf-43fa-9e9f-40d887f964cb",
   "metadata": {},
   "source": [
    "Consequently, the test statistics of the 2-sample Z-test for the difference in means can be calculated as follows:\n",
    "\n",
    "$$z=\\frac{\\hat \\mu_{con}-\\hat \\mu_{exp}}{\\sqrt{\\frac{\\sigma_{con}^2}{N_{con}}+\\frac{\\sigma_{exp}^2}{N_{exp}}}} \\text{ ~ } N(0,1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b31b1b-5539-4c6d-9d85-1573138c6672",
   "metadata": {},
   "source": [
    "$$\\large p_{value} ~~~~=~~~~ Pr[z\\leq Z \\text{ or } z\\geq Z]~~~=~~~~~ 2 \\times Pr[z \\geq Z]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d7d1f-9408-43b5-b602-e9b6b8e42863",
   "metadata": {},
   "source": [
    "$$CI = \\large \\left[ (\\mu_{con}-\\mu_{exp})-z_{1-\\frac{\\alpha}{2}}\\times SE\\text{ , }~~~~~~ (\\mu_{con}-\\mu_{exp})+z_{1-\\frac{\\alpha}{2}}\\times SE \\right]$$\n",
    "\n",
    "where the $z_{(1-\\alpha/2)}$ is the critical value of the test corresponding to the two-sided z-test with alpha significance level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79e1c8-1a8a-45b1-99d4-6276708446af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "N_con = 60\n",
    "N_exp = 60\n",
    "\n",
    "# Significance Level\n",
    "alpha = 0.05\n",
    "\n",
    "X_A = np.random.randint(100, size = N_con)\n",
    "X_B = np.random.randint(100, size = N_exp)\n",
    "\n",
    "# Calculating means of control and experimental groups\n",
    "mu_con = np.mean(X_A)\n",
    "mu_exp = np.mean(X_B)\n",
    "\n",
    "variance_con = np.var(X_A)\n",
    "variance_exp = np.var(X_B)\n",
    "\n",
    "# Pooled Variance\n",
    "pooled_variance = np.sqrt(variance_con/N_con + variance_exp/N_exp)\n",
    "\n",
    "# Test statistics\n",
    "T = (mu_con-mu_exp)/np.sqrt(variance_con/N_con + variance_exp/N_exp)\n",
    "\n",
    "# two sided test and using symmetry property of Normal distibution so we multiple with 2\n",
    "p_value = norm.sf(T)*2\n",
    "\n",
    "# Z-critical value\n",
    "Z_crit  = norm.ppf(1-alpha/2)\n",
    "\n",
    "# Margin of error\n",
    "m = Z_crit*pooled_variance\n",
    "\n",
    "# Confidence Interval\n",
    "CI = [(mu_con - mu_exp) - m, (mu_con - mu_exp) + m]\n",
    "\n",
    "\n",
    "print(\"Test Statistics stat: \", T)\n",
    "print(\"Z-critical: \", Z_crit)\n",
    "print(\"P_value: \", p_value)\n",
    "print(\"Confidence Interval of 2 sample Z-test for proportions: \", np.round(CI,2))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "z = np.arange(-3,3,  0.1)\n",
    "plt.plot(z, norm.pdf(z), label = 'Standard Normal Distribution',color = 'purple',linewidth = 2.5)\n",
    "plt.fill_between(z[z>Z_crit], norm.pdf(z[z>Z_crit]), label = 'Right Rejection Region',color ='y' )\n",
    "plt.fill_between(z[z<(-1)*Z_crit], norm.pdf(z[z<(-1)*Z_crit]), label = 'Left Rejection Region',color ='y' )\n",
    "plt.title(\"Two Sample Z-test rejection region\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac51cb4-2bf1-4620-bd8f-7ffd4e350212",
   "metadata": {},
   "source": [
    "### Statistical Significance vs Practical Significance\n",
    "- During the statistical analysis phase of the A/B testing, when a small p-value is detected, then we speak about statistical significance. However, statistical significance alone is not enough to make a recommendation about launching a feature or a product.\n",
    "\n",
    "- After statistical significance is detected, the next step is to understand whether there is a practical significance. This will help us to understand whether the detected difference in the performances of the two groups is large enough to justify the investment or if it's too small and making a launch decision not worthy of investment.\n",
    "\n",
    "- One way to determine whether the A/B test has practical significance is to use the Confidence Interval and compare its lower bound to the MDE (estimate of the economic significance). More specifically, if the lower bound of CI is larger than the MDE (delta), then you can state that you have a practical significance. For example, if the CI = [5%, 7.5%] and the MDE = 3% then you can conclude to have a practical significance since 5% > 3%.\n",
    "\n",
    "- Note that, you should also be aware of the width of the CI. Make sure it is not too big since too wide CI gives you an indication that the precision of your results is small and the results will not be generalizable to the entire population (External Validity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cdebab-b847-4597-91d9-cbf36ddfdd12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
